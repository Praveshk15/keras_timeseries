{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 layers\n",
      "melhor parametro: 1\n"
     ]
    }
   ],
   "source": [
    "# %load lstm_zimbrao.py\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "#import talib\n",
    "\n",
    "seed=7\n",
    "np.random.seed(seed)  # for reproducibility\n",
    "\n",
    "from processing import *\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from custom_callbacks import CriteriaStopping\n",
    "from keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "#from hyperbolic_nonlinearities import AdaptativeAssymetricBiHyperbolic, AdaptativeBiHyperbolic, AdaptativeHyperbolicReLU, AdaptativeHyperbolic, PELU\n",
    "#from keras.layers.advanced_activations import ParametricSoftplus, SReLU, PReLU, ELU, LeakyReLU, ThresholdedReLU\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# dataframe = pandas.read_csv('ibov_google_15jun2017_1min_15d.csv', sep = ',', usecols=[1],  engine='python', skiprows=8, decimal='.',header=None)\n",
    "# dataset = dataframe[1].tolist()\n",
    "train = pandas.read_csv('minidolar/train.csv', sep = ',',  engine='python', decimal='.',header=0)\n",
    "test = pandas.read_csv('minidolar/test.csv', sep = ',',  engine='python', decimal='.',header=0)\n",
    "#dataset = dataframe['fechamento'].tolist()\n",
    "\n",
    "train_shift = train['shift']\n",
    "train_target = train['f0']\n",
    "train_close = train[['v3','v7','v11','v15','v19','v23','v27','v31','v35','v39','v43','v47','v51','v55','v59','v63','v67','v71','v75','v79','v83','v87','v91','v95','v99','v103','v107','v111','v115','v119']]\n",
    "#para reduzir um sample (3781->3780) e ficar par para usar msm batch_size\n",
    "train_close = train_close[:-1]\n",
    "train_target = train_target[:-1]\n",
    "train_shift = train_shift[:-1]\n",
    "\n",
    "test_shift = test['shift']\n",
    "test_target = test['f0']\n",
    "test_close = test[['v3','v7','v11','v15','v19','v23','v27','v31','v35','v39','v43','v47','v51','v55','v59','v63','v67','v71','v75','v79','v83','v87','v91','v95','v99','v103','v107','v111','v115','v119']]\n",
    "\n",
    "batch_size = 20\n",
    "nb_epoch = 50\n",
    "patience = 50\n",
    "look_back = 7\n",
    "\n",
    "def evaluate_model(model, name, n_layers, ep):\n",
    "    X_train, X_test, Y_train, Y_test =  np.array(train_close),  np.array(test_close),  np.array(train_target.values.reshape(train_target.size,1)),  np.array(test_target.values.reshape(test_target.size,1))\n",
    "    X_trainp, X_testp, Y_trainp, Y_testp = X_train+train_shift.values.reshape(train_shift.size,1), X_test+test_shift.values.reshape(test_shift.size,1), Y_train+train_shift.values.reshape(train_shift.size,1), Y_test + test_shift.values.reshape(test_shift.size,1)\n",
    "\n",
    "\n",
    "    csv_logger = CSVLogger('output/%d_layers/%s.csv' % (n_layers, name))\n",
    "    es = EarlyStopping(monitor='loss', patience=patience)\n",
    "    #mcp = ModelCheckpoint('output/mnist_adaptative_%dx800/%s.checkpoint' % (n_layers, name), save_weights_only=True)\n",
    "    #tb = TensorBoard(log_dir='output/mnist_adaptative_%dx800' % n_layers, histogram_freq=1, write_graph=False, write_images=False)\n",
    "\n",
    "    \n",
    "    #sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "    #optimizer = sgd\n",
    "    optimizer = \"adam\"\n",
    "    #optimizer = \"adadelta\"\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    #X_train = np.expand_dims(X_train, axis=2)\n",
    "    #X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "    #history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=ep, verbose=0, validation_split=0.1, callbacks=[csv_logger,es])\n",
    "\n",
    "    for i in range(nb_epoch):\n",
    "\t    history = model.fit(X_train, Y_train, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "\t    model.reset_states()\n",
    "    #trainScore = model.evaluate(X_train, Y_train, verbose=0)\n",
    "    #print('Train Score: %f MSE (%f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
    "    #testScore = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    #print('Test Score: %f MSE (%f RMSE)' % (testScore, math.sqrt(testScore)))\n",
    "\n",
    "    # make predictions (scaled)\n",
    "    trainPredict = model.predict(X_train, batch_size=batch_size)\n",
    "    testPredict = model.predict(X_test, batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    # invert predictions (back to original)\n",
    "    new_predicted = testPredict+test_shift.values.reshape(test_shift.size,1)\n",
    "    new_train_predicted= trainPredict+train_shift.values.reshape(train_shift.size,1)\n",
    "\n",
    "    # calculate root mean squared error\n",
    "    trainScore = mean_squared_error(new_train_predicted, Y_trainp)\n",
    "    #print('Train Score: %f RMSE' % (trainScore))\n",
    "    testScore = mean_squared_error(new_predicted, Y_testp)\n",
    "    #print('Test Score: %f RMSE' % (testScore))\n",
    "    epochs = nb_epoch\n",
    "\n",
    "    # fig = plt.figure()\n",
    "    # plt.plot(Y_test[:150], color='black') # BLUE - trained RESULT\n",
    "    # plt.plot(testPredict[:150], color='blue') # RED - trained PREDICTION\n",
    "    #plt.plot(Y_testp[:150], color='green') # GREEN - actual RESULT\n",
    "    #plt.plot(new_predicted[:150], color='red') # ORANGE - restored PREDICTION\n",
    "    #plt.show()\n",
    "\n",
    "    return trainScore, testScore, epochs, optimizer\n",
    "\n",
    "\n",
    "n_layers = int(0)\n",
    "print(n_layers,'layers')\n",
    "\n",
    "#nonlinearities = ['aabh', 'abh', 'ah', 'sigmoid', 'relu', 'tanh']\n",
    "#nonlinearities = ['sigmoid', 'relu', 'tanh']\n",
    "nonlinearities = ['relu']\n",
    "\n",
    "with open(\"output/%d_layers/compare.csv\" % n_layers, \"a\") as fp:\n",
    "    fp.write(\"-MINIDOLAR/LSTM NN\\n\")\n",
    "\n",
    "hals = []\n",
    "\n",
    "TRAIN_SIZE = 30\n",
    "TARGET_TIME = 1\n",
    "LAG_SIZE = 1\n",
    "EMB_SIZE = 1\n",
    "HIDDEN_RNN = 4\n",
    "\n",
    "\n",
    "testScore_aux = 999999\n",
    "f_aux = 0\n",
    "\n",
    "for f in range(1,2):\n",
    "    name='relu'\n",
    "    model = Sequential()\n",
    "\n",
    "    #model.add(Dense(500, input_shape = (TRAIN_SIZE, )))\n",
    "    #model.add(Activation(name))\n",
    "\n",
    "    model.add(LSTM(batch_input_shape=(batch_size, TRAIN_SIZE, 1), \n",
    "    input_shape = (None, EMB_SIZE,), \n",
    "    units=HIDDEN_RNN, return_sequences=True, stateful=True))\n",
    "    n_layers = n_layers+1 #para que o input 0 seja realmente uma camada, 1 serem 2, etc\n",
    "    for l in range(n_layers):\n",
    "        if(l==n_layers-1):\n",
    "            model.add(LSTM(batch_input_shape=(batch_size, TRAIN_SIZE, 1), \n",
    "            input_shape = (None, EMB_SIZE,),\n",
    "            units=HIDDEN_RNN, return_sequences=False, stateful=True))\n",
    "        else:\n",
    "            model.add(LSTM(batch_input_shape=(batch_size, TRAIN_SIZE, 1), \n",
    "            input_shape = (None, EMB_SIZE,), \n",
    "            units=HIDDEN_RNN, return_sequences=True, stateful=True))\n",
    "\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    #model.summary()\n",
    "\n",
    "    trainScore, testScore, epochs, optimizer = evaluate_model(model, name, n_layers,nb_epoch)\n",
    "    if(testScore_aux > testScore):\n",
    "        testScore_aux=testScore\n",
    "        f_aux = f\n",
    "\n",
    "    elapsed_time = (time.time() - start_time)\n",
    "    with open(\"output/%d_layers/compare.csv\" % (n_layers-1), \"a\") as fp:\n",
    "        fp.write(\"%i,%s,%f,%f,%d,%s --%s seconds\\n\" % (f, name, trainScore, testScore, epochs, optimizer, elapsed_time))\n",
    "\n",
    "    model = None\n",
    "\n",
    "print(\"melhor parametro: %i\" % f_aux)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3780,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
